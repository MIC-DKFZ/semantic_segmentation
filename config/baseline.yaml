#@package _global_

# DEFAULT LIST
# defines which modules from which parameter group are used by default and in which order they are composed
defaults:
  - _self_
  - optional hyperparameters:
  - optimizer: SGD
  - lr_scheduler: polynomial
  - callbacks: default
  - data_augmentation: default
  - metric: mean_IoU
  - dataset: Cityscapes
  - model: hrnet
  - environment: local

  # USING COLORLOG PLUGIN OF HYDRA
  - override hydra/hydra_logging: colorlog
  - override hydra/job_logging: colorlog


# DEFAULT HYPERPARAMETERS
ORG_CWD: ${hydra:runtime.cwd}   # Saving the original working dir
experiment: "baseline"          # possibility for naming the experiment
LOGDIR: logs/                   # Default logging directory
# FOR CITYSCAPES AND VOC2010_CONTEXT THE HYPERPARAMETERS in config/hyperparameters/*.yaml are used
# SO EDDIT THEM THERE IF YOU ARE USING THESE DATASETS
num_workers: 10                 # number of workers for dataloader
batch_size: 6                   # batch size per gpu for training
val_batch_size: ${batch_size}   # batch size per gpu for validation
epochs: 400                     # number of eposchs
lr: 0.01                        # learning rate for training (0.01445439770745928)
momentum: 0.9                   # momentum for optimizer
weight_decay: 0.0005            # wd for optimizer
lossfunction: [ "CE", "CE", "CE", "CE"]     # list of lossfunctions
lossweight:   [1.0, 0.4, 0.05, 0.05]        # corresponding weights for each loss function
pl_trainer:                     # parameters for the pytorch lightning trainer
  max_epochs: ${epochs}         # parsing the number of epochs which is defined as a hyperparameter
  gpus: -1                      # using all available GPUs
  precision: 16                 # using Mixed Precision
  sync_batchnorm: True          # using synchronized batch normalization for multi gpu training
  benchmark: True               # using benchmark for faster training
  enable_checkpointing: False   # Enable/Disable Checkpointing

  # Some usefull pl parameters for debugging
  #limit_train_batches: 0.25
  #limit_val_batches: 0.25
  #limit_test_batches: 0.25
  #accumulate_grad_batches: 2

#SOME CUSTOMIZATIONS OF HYDRA
hydra:
  output_subdir: hydra
  run:
    dir: ${LOGDIR}/${DATASET.NAME}/${MODEL.NAME}/${experiment}__${path_formatter:${hydra.job.override_dirname}}/${now:%Y-%m-%d_%H-%M-%S}
    # Example Dir: /.../Semantic_Segmentation/logs/Cityscapes/hrnet/baseline__lossfunction_CE/2022-02-14_15-42-43/<ouputs>
    # using path_formatter with is defined in main.py to resolve problems which may occur with characters like [,],",or ',' in paths
  sweep:
    dir: ${hydra.run.dir}_mr
    subdir: ${hydra.job.num}
  job:
    config:
      override_dirname:
        kv_sep: "_"         # dont use "=" to prevent problems when parsing paths
        item_sep: "__"
        exclude_keys:       # excluding some key from ${hydra.job.override_dirname}
          - model           # already used in the path
          - dataset         # already used in the path
          - environment     # no needed information for the experiments
          - finetune_from   # to long
          - continue_from   # to long
          - experiment      # already used in the path
          - LOGDIR          # no needed information for the experiments

#DEFINING THE DATAMODULE
datamodule:
  _target_: datasets.DataModules.BaseDataModule   #Base Data Module
  num_workers: ${num_workers}             # Parsing all needed hyperparameters
  batch_size: ${batch_size}
  val_batch_size: ${val_batch_size}
  augmentations: ${AUGMENTATIONS}
  dataset: ${dataset}
#DEFINING THE SAVING BEHAVIOR, ONLY USED WHEN CHECKPOINTING IS ENABLED
ModelCheckpoint:
  _target_: utils.callbacks.customModelCheckpoint   #cusotom checkpointing
  monitor: "metric/${METRIC.NAME}"        # name of the metric during logging
  mode: "max"                             # min or max: should be metric me maximised or minimized
  filename: 'best_epoch_{epoch}__${METRIC.NAME}_{metric/${METRIC.NAME}:.4f}'
  auto_insert_metric_name: False          # needs to be false for better naming of checkpoint
  save_last: True                         # if the last checkpoint should be saved too