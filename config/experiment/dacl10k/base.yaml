# @package _global_
defaults:
  - override /augmentation@augmentation.train: randaugmentv3_flip
  - override /augmentation@augmentation.val: norm
  - override /augmentation@augmentation.test: norm
  - override /dataset: dacl10k
  - override /model: Maxvit_512_xlarge
  - override /trainer: SemSegML
  - override /dataclass: ML_TrainVal
  - override /label_handler: multilabel_dacl
  - override /metric: mean_IoU_Class_Multilabel
  - override /optimizer: MADGRAD

# Configure the augmentation pipeline
augmentation:
  cfg:
    resize_size: [512,512]
    scale_limit: null
    crop_size: null
    pad_size: null
    pad_mode: 0
    pad_val: 0
    pad_mask: 0
    mean: [ 0.485, 0.456, 0.406 ]
    std: [ 0.229, 0.224, 0.225 ]
    M: 3
    N: 3

# Test Time Augmentations
tta:
  scales: [ 1]
  hflip: True

#Hyperparameters
batch_size: 4                   # batch size per gpu for training
val_batch_size: ${batch_size}   # batch size per gpu for validation
epochs: 65                      # number of epochs
lr: 0.0002                        # learning rate for training (0.01445439770745928)
momentum: 0.9                   # momentum for optimizer
weight_decay: 0.            # wd for optimizer
loss:
  function: [ "mlJL","mlJL","mlJL","mlJL" ]     # list of lossfunctions
  val_function: [ "mlJL","mlJL","mlJL","mlJL" ]     # list of lossfunctions
  weight:   [1.0, 0.4, 0.05, 0.05]            # corresponding weights for each loss function