# @package _global_
num_workers: 10                 #number of workers for dataloader
batch_size: 6                   #batch size per gpu for training
val_batch_size: ${batch_size}   #batch size per gpu for validation
epochs: 400                     #number of eposchs
lr: 0.01                        #learning rate for training (0.01445439770745928)
lossfunction: [ "wCE", "wCE", "wCE", "wCE"]     #list of lossfunctions
lossweight:   [1.0, 0.4, 0.05, 0.05]            #corresponding weights for each loss function
momentum: 0.9                   #momentum for optimizer
weight_decay: 0.0005            #wd for optimizer