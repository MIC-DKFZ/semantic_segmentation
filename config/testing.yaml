#@package _global_

# DEFAULT LIST
# defines which modules from which parameter group are used by default and in which order they are composed
defaults:
  - _self_
  - hyperparameters: #default
  - callbacks: default
  - data_augmentation: default
  - metric: mean_IoU
  - dataset: Cityscapes
  - model: hrnet
  - environment: local

  # USING COLORLOG PLUGIN OF HYDRA
  - override hydra/hydra_logging: colorlog
  - override hydra/job_logging: colorlog

ckpt_dir: ???
ORG_CWD: ${hydra:runtime.cwd}
# DEFAULT HYPERPARAMETERS
#ORG_CWD: ${hydra:runtime.cwd}   # Saving the original working dir
#experiment: "baseline"          # possibility for naming the experiment
#LOGDIR: logs/                   # Default logging directory
# FOR CITYSCAPES AND VOC2010_CONTEXT THE HYPERPARAMETERS in config/hyperparameters/*.yaml are used
# SO EDDIT THEM THERE IF YOU ARE USING THESE DATASETS
#num_workers: 10                 # number of workers for dataloader
#batch_size: 6                   # batch size per gpu for training
#val_batch_size: ${batch_size}   # batch size per gpu for validation
#epochs: 400                     # number of eposchs
#lr: 0.01                        # learning rate for training (0.01445439770745928)
#momentum: 0.9                   # momentum for optimizer
#weight_decay: 0.0005            # wd for optimizer
#lossfunction: [ "CE", "CE", "CE", "CE"]     # list of lossfunctions
#lossweight:   [1.0, 0.4, 0.05, 0.05]        # corresponding weights for each loss function
#pl_trainer:                     # parameters for the pytorch lightning trainer
#  max_epochs: ${epochs}         # parsing the number of epochs which is defined as a hyperparameter
#  gpus: -1                      # using all available GPUs
#  precision: 16                 # using Mixed Precision
#  sync_batchnorm: True          # using synchronized batch normalization for multi gpu training
#  benchmark: True               # using benchmark for faster training
#  enable_checkpointing: False   # Enable/Disable Checkpointing

  # Some usefull pl parameters for debugging
  #limit_train_batches: 0.25
  #limit_val_batches: 0.25
  #limit_test_batches: 0.25
  #accumulate_grad_batches: 2

#SOME CUSTOMIZATIONS OF HYDRA
hydra:
  output_subdir: validation/hydra
  run:
    dir: ${ckpt_dir}
    # Example Dir: /.../Semantic_Segmentation/logs/Cityscapes/hrnet/baseline__lossfunction_CE/2022-02-14_15-42-43/<ouputs>
    # using path_formatter with is defined in main.py to resolve problems which may occur with characters like [,],",or ',' in paths
  sweep:
    dir: ${hydra.run.dir}_mr
    subdir: ${hydra.job.num}
  job_logging:
    handlers:
      file:
        #filename: validation/${hydra.job.name}.log
        filename: ${hydra.job.name}.log


#DEFINING THE DATAMODULE
datamodule:
  _target_: datasets.DataModules.BaseDataModule   #Base Data Module
  num_workers: ${num_workers}             # Parsing all needed hyperparameters
  batch_size: ${batch_size}
  val_batch_size: ${val_batch_size}
  augmentations: ${AUGMENTATIONS}
  dataset: ${dataset}

