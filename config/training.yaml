#@package _global_

# Default List
# defines which modules from which parameter group are used by default and in which order they are composed
defaults:
  - _self_
  - trainer: SemSeg             # Which Trainer to use
  - metric: mean_IoU            # Metric configuration
  - model: hrnet_ocr                # Model
  - dataset: Cityscapes/base         # Dataset
  - optimizer: SGD              # Optimizer
  - lr_scheduler: polynomial    # Learning rate scheduler
  - callbacks: default          # Callbacks
  - logger: tensorboard         # Logger
  - augmentation@augmentation.train: norm  # Data Augmentation
  - augmentation@augmentation.test: norm  # Data Augmentation
  - augmentation@augmentation.val: norm  # Data Augmentation
  - experiment/default          # Load Default Hyperparameters
  - optional experiment:
  - environment: local          # Environment

  - override hydra/hydra_logging: colorlog    # Using colorlog plugin of hydra for logging
  - override hydra/job_logging: colorlog

# Logging Related Parameters
ORG_CWD: ${hydra:runtime.cwd}   # Saving the original working dir
OUTPUT_DIR: ${hydra:runtime.output_dir}
name: "run"                     # Possibility for naming the experiment
LOGDIR: logs/                   # Default logging directory
num_example_predictions: 2      # Save some example predictions during validation/testing
                                # Just for visualization/inspection, set to 0 if not wanted

# Defining the datamodule
datamodule:
  _target_: datasets.DataModules.BaseDataModule   # Base Data Module
  num_workers: ${num_workers}      # Parsing all needed experiment
  batch_size: ${batch_size}
  val_batch_size: ${val_batch_size}
  augmentations: ${augmentation} # Parsing the Data augmentation, defined in the augmentation config
  dataset: ${dataset}             # Parsing the Dataset defined in the dataset config

# Defining the saving behavior. Only used when checkpointing is enabled in the pl_trainer
ModelCheckpoint:
  _target_: src.callbacks.callbacks.customModelCheckpoint   # Custom checkpoint Callback with a few modifications
  monitor: "metric/${METRIC.NAME}"        # Name of the metric during logging
  mode: "max"                             # min or max: should be metric me maximised or minimized
  filename: 'best_epoch_{epoch}__${METRIC.NAME}_{metric/${METRIC.NAME}:.4f}'
  auto_insert_metric_name: False          # Needs to be false for better naming of checkpoint
  save_last: True                         # If the last checkpoint should be saved too

# Customizations of Hydra
hydra:
  output_subdir: hydra
  run:
    dir: ${LOGDIR}/${DATASET.NAME}/${MODEL.NAME}/${name}__${path_formatter:${hydra.job.override_dirname}}/${fold_formatter:${dataset}}/${now:%Y-%m-%d_%H-%M-%S}
    # Example Dir: /.../Semantic_Segmentation/logs/Cityscapes/hrnet/baseline__lossfunction_CE/2022-02-14_15-42-43/<ouputs>
    # using path_formatter with is defined in training.py to resolve problems which may occur with characters like [,],",or ',' in paths
  sweep:
    dir: multi_run_${hydra.run.dir}
    subdir: ${hydra.job.num}
  job_logging:
    handlers:
      file:
        filename: ${hydra.runtime.output_dir}/${hydra.job.name}.log
  job:
    config:
      override_dirname:
        kv_sep: "_"         # do not use "=" to prevent problems when parsing paths
        item_sep: "__"
        exclude_keys:       # excluding some key from ${hydra.job.override_dirname}
          - model           # already used in the path
          - dataset         # already used in the path
          - environment     # no needed information for the experiments
          - finetune_from   # to long
          - continue_from   # to long
          - name            # already used in the path
          - LOGDIR          # no needed information for the experiments
          - pl_trainer.enable_checkpointing
          - dataset.fold